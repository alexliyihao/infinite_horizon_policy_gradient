{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the path\n",
    "PATH = \"/content/drive/My Drive/Columbia_Sept_19_/Class/IEOR research/infinite_horizon_policy_gradient/NN_model_implementation\"\n",
    "import sys\n",
    "sys.path.append(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from implementations import *\n",
    "from optimization_grad import *\n",
    "from variance_grad import *\n",
    "from analysis import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorboard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import os\n",
    "logs_base_dir = \"runs\"\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir \"runs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "environment = riverswim()\n",
    "\n",
    "# policies\n",
    "policy_benchmark = Policy().cuda()\n",
    "policy_causality = copy.deepcopy(policy_benchmark).cuda()\n",
    "policy_baseline  = copy.deepcopy(policy_benchmark).cuda()\n",
    "policy_ihp1      = copy.deepcopy(policy_benchmark).cuda()\n",
    "# configuration\n",
    "rolling_out_start_recording = 1000\n",
    "rolling_out_step = 15000\n",
    "n_rolling_out = 5\n",
    "optimization_step = 50\n",
    "lr = 0.25\n",
    "\n",
    "#visualization\n",
    "tensorboard = SummaryWriter()\n",
    "reward_list_in_policy = []\n",
    "for time_step in tqdm(range(optimization_step), desc = \"Optimization step\", leave = True):\n",
    "\n",
    "    policy_grad_benchmark, reward_benchmark = gradient_estimate_benchmark(policy_benchmark,\n",
    "                                                                          policy_benchmark,\n",
    "                                                                          environment,\n",
    "                                                                          rolling_out_start_recording,\n",
    "                                                                          rolling_out_step,\n",
    "                                                                          n_rolling_out)\n",
    "    update_parameter(policy = policy_benchmark, \n",
    "                     policy_gradient = policy_grad_benchmark, \n",
    "                     learning_rate= lr,\n",
    "                     normalize = True)\n",
    "    \n",
    "    policy_grad_causality, reward_causality = gradient_estimate_causality(policy_causality,\n",
    "                                                                          policy_causality,\n",
    "                                                                          environment,\n",
    "                                                                          rolling_out_start_recording,\n",
    "                                                                          rolling_out_step,\n",
    "                                                                          n_rolling_out)\n",
    "    update_parameter(policy = policy_causality, \n",
    "                     policy_gradient = policy_grad_causality, \n",
    "                     learning_rate= lr,\n",
    "                     normalize = True)\n",
    "    \n",
    "    policy_grad_baseline, reward_baseline = gradient_estimate_baseline(policy_baseline,\n",
    "                                                                        policy_baseline,\n",
    "                                                                        environment,\n",
    "                                                                        rolling_out_start_recording,\n",
    "                                                                        rolling_out_step,\n",
    "                                                                        n_rolling_out)\n",
    "    update_parameter(policy = policy_baseline, \n",
    "                     policy_gradient = policy_grad_baseline, \n",
    "                     learning_rate= lr,\n",
    "                     normalize = True)\n",
    "    \n",
    "    policy_grad_ihp1, reward_ihp1 = gradient_estimate_ihp1(policy_ihp1,\n",
    "                                                           policy_ihp1,\n",
    "                                                           environment,\n",
    "                                                           rolling_out_start_recording,\n",
    "                                                           rolling_out_step,\n",
    "                                                           n_rolling_out,\n",
    "                                                           in_policy = True)\n",
    "    update_parameter(policy = policy_ihp1, \n",
    "                     policy_gradient = policy_grad_ihp1, \n",
    "                     learning_rate= lr,\n",
    "                     normalize = True)\n",
    "    \n",
    "    tensorboard.add_scalars(main_tag=\"Average Reward Per Step: In-Policy\", \n",
    "                            tag_scalar_dict={\"Benchmark\": reward_benchmark,\n",
    "                                             \"Causality\": reward_causality,\n",
    "                                             \"Baseline\": reward_baseline,\n",
    "                                             \"ihp1\": reward_ihp1},\n",
    "                            global_step = time_step)\n",
    "    reward_list_in_policy.append([reward_benchmark, reward_causality, reward_baseline, reward_ihp1])\n",
    "\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_array_np = np.array(reward_list_in_policy)\n",
    "df = pd.DataFrame(reward_array_np, columns= [\"Benchmark\", \"Causality\", \"Baseline\", \"IHP\"])\n",
    "sns.lineplot(data = df, dashes=False)\n",
    "plt.title(\"Optimization Procedure: In-policy setting\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average Reward Per Step\")\n",
    "plt.savefig(\"in-policy_optimization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Off-policy Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "environment = riverswim()\n",
    "\n",
    "# policies\n",
    "policy_0 = Policy().cuda()\n",
    "policy_benchmark = copy.deepcopy(policy_0).cuda()\n",
    "policy_causality = copy.deepcopy(policy_0).cuda()\n",
    "policy_baseline  = copy.deepcopy(policy_0).cuda()\n",
    "policy_ihp1      = copy.deepcopy(policy_0).cuda()\n",
    "# configuration\n",
    "rolling_out_start_recording = 1000\n",
    "rolling_out_step = 15000\n",
    "n_rolling_out = 5\n",
    "optimization_step = 100\n",
    "lr = 0.15\n",
    "\n",
    "reward_list_off_policy = []\n",
    "\n",
    "#visualization\n",
    "tensorboard = SummaryWriter()\n",
    "\n",
    "for time_step in tqdm(range(optimization_step), desc = \"Optimization step\", leave = True):\n",
    "\n",
    "\n",
    "    policy_grad_benchmark, policy_grad_causality,policy_grad_baseline, policy_grad_ihp1 = gradient_estimate_aggregate(policy_0, \n",
    "                                                                                                                      policy_benchmark,\n",
    "                                                                                                                      policy_causality,\n",
    "                                                                                                                      policy_baseline,\n",
    "                                                                                                                      policy_ihp1,\n",
    "                                                                                                                      environment, \n",
    "                                                                                                                      rolling_out_start_recording, \n",
    "                                                                                                                      rolling_out_step, \n",
    "                                                                                                                      n_rolling_out)\n",
    "    update_parameter(policy = policy_benchmark, \n",
    "                     policy_gradient = policy_grad_benchmark, \n",
    "                     learning_rate= lr,\n",
    "                     normalize = True)\n",
    "    \n",
    "    update_parameter(policy = policy_causality, \n",
    "                     policy_gradient = policy_grad_causality, \n",
    "                     learning_rate= lr,\n",
    "                     normalize = True)\n",
    "    \n",
    "\n",
    "    update_parameter(policy = policy_baseline, \n",
    "                     policy_gradient = policy_grad_baseline, \n",
    "                     learning_rate= lr,\n",
    "                     normalize = True)\n",
    "    \n",
    "    update_parameter(policy = policy_ihp1, \n",
    "                     policy_gradient = policy_grad_ihp1, \n",
    "                     learning_rate= lr,\n",
    "                     normalize = True)\n",
    "    \n",
    "    reward_benchmark = roll_out_evaluate_average(policy_benchmark, environment, rolling_out_start_recording, rolling_out_step)\n",
    "    reward_causality = roll_out_evaluate_average(policy_causality, environment, rolling_out_start_recording, rolling_out_step)\n",
    "    reward_baseline = roll_out_evaluate_average(policy_baseline, environment, rolling_out_start_recording, rolling_out_step)\n",
    "    reward_ihp1 = roll_out_evaluate_average(policy_ihp1, environment, rolling_out_start_recording, rolling_out_step)\n",
    "    \n",
    "    tensorboard.add_scalars(main_tag=\"Average reward per step: off policy\", \n",
    "                            tag_scalar_dict={\"Benchmark\": reward_benchmark,\n",
    "                                             \"Causality\": reward_causality,\n",
    "                                             \"Baseline\": reward_baseline,\n",
    "                                             \"ihp1\": reward_ihp1},\n",
    "                            global_step = time_step)\n",
    "    reward_list_off_policy.append([reward_benchmark, reward_causality, reward_baseline, reward_ihp1])\n",
    "    \n",
    "\n",
    "tensorboard.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_array_np = np.array(reward_list_in_policy)\n",
    "df = pd.DataFrame(reward_array_np, columns= [\"Benchmark\", \"Causality\", \"Baseline\", \"IHP\"])\n",
    "sns.lineplot(data = df, dashes=False)\n",
    "plt.title(\"Optimization Procedure: Off-policy setting\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Average Reward Per Step\")\n",
    "plt.savefig(\"Off-policy_optimization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individually Rolling Out Variance Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = riverswim()\n",
    "\n",
    "# policies\n",
    "policy_0 = Policy().cuda()\n",
    "policy_1 = copy.deepcopy(policy_0).cuda()\n",
    "#policy_1 = Policy().cuda()\n",
    "\n",
    "# configuration\n",
    "rolling_out_start_recording = 1000\n",
    "rolling_out_step = 15000\n",
    "n_rolling_out = 100\n",
    "ihp1_record = variance_estimate_ihp1(policy_0 = policy_0, \n",
    "                                     policy_1 = policy_1, \n",
    "                                     env = env, \n",
    "                                     rolling_out_start_recording = rolling_out_start_recording, \n",
    "                                     rolling_out_step = rolling_out_step, \n",
    "                                     n_rolling_out = n_rolling_out\n",
    "                                    )\n",
    "benchmark_record = variance_estimate_benchmark(policy_0 = policy_0, \n",
    "                                               policy_1 = policy_1, \n",
    "                                               env = env, \n",
    "                                               rolling_out_start_recording = rolling_out_start_recording, \n",
    "                                               rolling_out_step = rolling_out_step, \n",
    "                                               n_rolling_out = n_rolling_out\n",
    "                                               )\n",
    "baseline_record = variance_estimate_baseline(policy_0 = policy_0, \n",
    "                                                                   policy_1 = policy_1, \n",
    "                                                                   env = env, \n",
    "                                                                   rolling_out_start_recording = rolling_out_start_recording, \n",
    "                                                                   rolling_out_step = rolling_out_step, \n",
    "                                                                   n_rolling_out = n_rolling_out\n",
    "                                                                   )\n",
    "causality_record = variance_estimate_causality(policy_0 = policy_0, \n",
    "                                               policy_1 = policy_1, \n",
    "                                               env = env, \n",
    "                                               rolling_out_start_recording = rolling_out_start_recording, \n",
    "                                               rolling_out_step = rolling_out_step, \n",
    "                                               n_rolling_out = n_rolling_out\n",
    "                                               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis(benchmark_record, causality_record, baseline_record, ihp1_record)\n",
    "boxplot_analysis(benchmark_record, causality_record, baseline_record, ihp1_record, np.random.randint(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Random Number Variance Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment\n",
    "env = riverswim()\n",
    "\n",
    "# policies\n",
    "policy_0 = Policy().cuda()\n",
    "policy_1 = copy.deepcopy(policy_0).cuda()\n",
    "#policy_1 = Policy().cuda()\n",
    "\n",
    "# configuration\n",
    "rolling_out_start_recording = 1000\n",
    "rolling_out_step = 15000\n",
    "n_rolling_out = 100\n",
    "benchmark_record, causality_record, baseline_record, ihp1_record = variance_estimate_aggregate(policy_0 = policy_0, \n",
    "                                                                                               policy_1 = policy_1, \n",
    "                                                                                               env = env, \n",
    "                                                                                               rolling_out_start_recording = rolling_out_start_recording, \n",
    "                                                                                               rolling_out_step = rolling_out_step, \n",
    "                                                                                               n_rolling_out = n_rolling_out\n",
    "                                                                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis(benchmark_record, causality_record, baseline_record, ihp1_record)\n",
    "boxplot_analysis(benchmark_record, causality_record, baseline_record, ihp1_record, np.random.randint(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
